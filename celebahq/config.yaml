loss_function:
  generator_loss:
    _target_: src.loss.RelativisticGeneratorLoss
    style_reconstruction:
      _target_: src.loss.StyleRecontructionLoss
    style_diversity:
      _target_: src.loss.StyleDiversityLoss
    cycle:
      _target_: src.loss.CycleLoss
    style_reconstruction_coef: 1
    cycle_coef: 1
    style_diversity_coef: 0.3
    n_epoch: ${trainer.n_epochs}
  discriminator_loss:
    _target_: src.loss.RelativisticDiscriminatorLoss
    beta: 1
datasets:
  train:
    _target_: src.datasets.HQDataset
    split: source
    instance_transforms: ${transforms.instance_transforms.source}
  reference:
    _target_: src.datasets.HQDataset
    split: reference
    instance_transforms: ${transforms.instance_transforms.reference}
  val:
    _target_: src.datasets.HQDataset
    split: val
    instance_transforms: ${transforms.instance_transforms.val}
dataloader:
  train:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    drop_last: true
    pin_memory: true
    num_workers: 3
    pin_memory_device: cuda
    collate_fn: src.datasets.collate_fn
  reference:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    drop_last: true
    pin_memory: true
    num_workers: 3
    pin_memory_device: cuda
    collate_fn: src.datasets.ref_collate_fn
  val:
    _target_: torch.utils.data.DataLoader
    batch_size: 32
    drop_last: false
    pin_memory: false
    num_workers: 2
    pin_memory_device: cuda
    collate_fn: src.datasets.collate_fn
metrics:
  train:
  - _target_: src.metrics.IsMetric
    device: ${device}
  - _target_: src.metrics.FidMetric
    device: ${device}
  - _target_: src.metrics.LpipsMetric
    device: ${device}
  eval:
  - _target_: src.metrics.IsMetric
    device: ${device}
  - _target_: src.metrics.FidMetric
    device: ${device}
  - _target_: src.metrics.LpipsMetric
    device: ${device}
model:
  model:
    _target_: src.model.StarGAN
    spectr_norm: false
    generator:
      _target_: src.model.Generator
      scale_factor: 2
      downsampling_block_size_expand: 3
      downsampling_block_size_no_expand: 1
      middle_block_size: 4
      initial_hidden_channels: 64
      embedding_dim: ${embedding_size}
    mapping_network:
      _target_: src.model.MappingNetwork
      latent_dim: ${latent_code_size}
      hidden_dim: 512
      embedding_dim: ${embedding_size}
      shared_size: 4
      unshared_size: 4
      domains: ${num_domains}
    style_encoder:
      _target_: src.model.ResNet
      scale_factor: 2
      downsampling_block_size_expand: 3
      downsampling_block_size_no_expand: 3
      initial_hidden_channels: 64
      out_dim: ${embedding_size}
      domains: ${num_domains}
      compression_kernel: 4
    discriminator:
      _target_: src.model.ResNet
      scale_factor: 2
      downsampling_block_size_expand: 3
      downsampling_block_size_no_expand: 3
      initial_hidden_channels: 64
      out_dim: 1
      domains: ${num_domains}
      compression_kernel: 4
optimizer:
  g_optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    betas: ${betas}
    fused: true
    weight_decay: 0.0001
  e_optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    betas: ${betas}
    fused: true
    weight_decay: 0.0001
  d_optimizer:
    _target_: torch.optim.Adam
    lr: 0.0002
    betas: ${betas}
    fused: true
    weight_decay: 0.0001
  f_optimizer:
    _target_: torch.optim.Adam
    lr: 1.0e-06
    betas: ${betas}
    fused: true
    weight_decay: 0.0001
transforms:
  base_transforms:
  - _target_: torchvision.transforms.v2.RandomResizedCrop
    size: ${img_size}
    scale:
    - 0.8
    - 1.0
    ratio:
    - 0.9
    - 1.1
  - _target_: torchvision.transforms.v2.RandomHorizontalFlip
  - _target_: torchvision.transforms.v2.ToTensor
  - _target_: torchvision.transforms.v2.Normalize
    mean:
    - 0.5
    - 0.5
    - 0.5
    std:
    - 0.5
    - 0.5
    - 0.5
  instance_transforms:
    source:
      data_object:
        _target_: torchvision.transforms.v2.Compose
        transforms: ${transforms.base_transforms}
    reference:
      data_object:
        _target_: torchvision.transforms.v2.Compose
        transforms: ${transforms.base_transforms}
      reference_object:
        _target_: torchvision.transforms.v2.Compose
        transforms: ${transforms.base_transforms}
    val:
      data_object:
        _target_: torchvision.transforms.v2.Compose
        transforms:
        - _target_: torchvision.transforms.v2.Resize
          size: ${img_size}
        - _target_: torchvision.transforms.v2.ToTensor
        - _target_: torchvision.transforms.v2.Normalize
          mean:
          - 0.5
          - 0.5
          - 0.5
          std:
          - 0.5
          - 0.5
          - 0.5
writer:
  _target_: src.logger.WandBWriter
  project_name: StarGAN-v2 CelebaHQ
  entity: null
  run_name: relativistic discriminator up
  mode: online
  loss_names:
  - generator_loss
  - discriminator_loss
  - g_adversarial_loss
  - g_style_reconstruction_loss
  - g_style_diversityn_loss
  - g_cycle_loss
  - d_adversarial_loss
  log_checkpoints: false
  id_length: 8
  run_id: mhmz1ruh
img_size: 256
num_domains: 2
embedding_size: 64
latent_code_size: 16
initial_decay: 0.4
final_decay: 0.99
ema_step: 100
betas:
- 0.0
- 0.99
device: cuda
balanced: true
trainer:
  seed: null
  deterministic: false
  device: ${device}
  save_dir: expiriments
  override: false
  n_epochs: 40
  save_period: 3
  resume_from: checkpoint-epoch33.pth
  log_step: 500
  discriminator_steps: 1
